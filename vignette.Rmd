---
title: "vignette"
output: html_document
date: "2023-12-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Binary Classification Vignette

For the purpose of this vignette, we will use data from the National Institute of Diabetes and Digestive Kidney Disease.

Activity: ...
Open a new RScript and copy-paste the chunk below to load the data
```{r}

# load packages
library(readr)
library(vip)
library(naniar)
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor)
library(kableExtra)
library(yardstick)
library(kknn)
library(corrplot)
library(themis)
library(dplyr)
library(ggplot2)
library(scales)
library(rpart.plot)
library(discrim)
library(klaR)
library(plotly)
library(ranger)
library(xgboost)
library(recipes)
library(ROSE)
library(randomForest)
tidymodels_prefer()

# read data
db <- read_csv("data/diabetes.csv")

```


Data Partitioning
First we will do cross-validation and data splitting.  
We then partition the diabetes data into training and test sets.
```{r}
set.seed(3435)
db_split <- initial_split(db, prop = 0.80,
                               strata = "Outcome")
db_train <- training(db_split)
db_test <- testing(db_split)
db_fold <- vfold_cv(db_train,v=4)
```

```{r}
db_train$Outcome <- factor(db_train$Outcome)
db_recipe_demo <- recipe(Outcome ~ ., data = db_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  step_upsample(Outcome, over_ratio = 0.5, skip = FALSE)

prep(db_recipe_demo) %>% bake(new_data = db_train) %>% 
  group_by(Outcome) %>% 
  summarise(count = n())
```



Binary Classification Algorithm #1 
Logistic Regression

```{r}
# Logistic regression
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

db_log_wflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(db_recipe)

# Tune the model
db_tune_reg <- tune_grid(
  object = db_log_wflow, 
  resamples = db_fold
)
```


Logistic Model Fitting
```{r}
# Fit the model
log_fit <- fit(db_log_wflow, db_train)
```


```{r}
# Extract predictions
log_preds <- augment(log_fit, new_data = db_train)

# Calculate ROC AUC directly using pROC
roc_curve <- roc(log_preds$Outcome, log_preds$.pred_0)
roc_auc_value <- auc(roc_curve)
print(roc_auc_value)

# Confusion Matrix
conf_matrix <- log_preds %>%
  conf_mat(truth = Outcome, estimate = .pred_class)

# Plot Confusion Matrix
conf_matrix %>% autoplot(type = 'heatmap') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# Show best tuning parameters
show_best(db_tune_reg, n = 1)

```


Accuracy Measures
```{r}
# Extract predictions
log_preds <- augment(log_fit, new_data = db_train)
```

Compute accuracy, precision, recall, F1 score
```{r}
# Calculate ROC AUC directly using pROC
roc_curve <- roc(log_preds$Outcome, log_preds$.pred_0)
roc_auc_value <- auc(roc_curve)
print(roc_auc_value)

# Confusion Matrix
conf_matrix <- log_preds %>%
  conf_mat(truth = Outcome, estimate = .pred_class)

# Plot Confusion Matrix
conf_matrix %>% autoplot(type = 'heatmap') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# Show best tuning parameters
show_best(db_tune_reg, n = 1)

```


Binary Classification Algorithm #2
Random Forest

```{r}

```






Binary Classification Algorithm #3
K-Nearest Neighbors

```{r}
# Recipe without step_upsample
db_recipe <- recipe(Outcome ~ ., data = db_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())

# Prepare the recipe
prep(db_recipe)%>% bake(new_data = db_train) %>% 
  group_by(Outcome) %>% 
  summarise(count = n())

 

# Define the k-NN model
knn_model <- nearest_neighbor(neighbors = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

# Create a workflow with the recipe and model
db_knn_wflow <- workflow() %>% 
  add_recipe(db_recipe) %>%  # Include the recipe
  add_model(knn_model)      # Include the model

# Grid for tuning
neighbors_grid <- grid_regular(neighbors(range = c(1, 10)), levels = 10)

# Tune the model
db_tune_knn <- tune_grid(
  object = db_knn_wflow, 
  resamples = db_fold, 
  grid = neighbors_grid
)

# Select the best model
best_knn_db <- select_best(
  db_tune_knn,
  metric = "roc_auc",
  neighbors
)

```

```{r}
best_knn_wf <- finalize_workflow(db_knn_wflow, best_knn_db)
knn_fit <- fit(best_knn_wf, data = db_train)

# Extract predictions
knn_preds <- augment(knn_fit, new_data = db_train)

# Calculate ROC AUC directly using pROC
roc_curve <- roc(knn_preds$Outcome, knn_preds$.pred_0)
roc_auc_value <- auc(roc_curve)
print(roc_auc_value)

# Confusion Matrix
conf_matrix <- knn_preds %>%
  conf_mat(truth = Outcome, estimate = .pred_class)

# Plot Confusion Matrix
conf_matrix %>% autoplot(type = 'heatmap') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

autoplot(db_tune_knn) + theme_minimal()
```

Binary Classification Algorithm #4
XGBoost


Binary Classification Algorithm #5
Boosted Tree Model


